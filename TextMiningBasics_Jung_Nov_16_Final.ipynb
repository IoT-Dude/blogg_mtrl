{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83484256-4dd8-4266-ab4d-e17304ae2fc3",
   "metadata": {},
   "source": [
    "2022-11-11-Jung-Archetypes-Analytical-Project-Text-Mining-I-II (Work in progress last updated on November 16, 2022 by PAX)\n",
    "\n",
    "The Jung Archetypes Analytical Project (JAAP)\n",
    "<br>\n",
    "This is a part of the Jung Archetypes Analytical Project where the hypothesis is that having enough data and crunching power one can make an inference engine useful in social dynamic environments.\n",
    "As  the idea of Jungs archetypes is fairly universal, it could be applied to a brand, and may be also effective as a tool for other knowledge actors  focusing on the knowledge efforts(such as academia etc). \n",
    "<br>\n",
    "This hypothesis is a proposition made as a basis for reasoning, without any assumption of truth. A supposition made on the basis of limited knowledge and evidence as a starting point for further investigation.\n",
    "\n",
    "In other words: this is a modest beginner adventure in the AI world where i will build a tool using textanalysis for Jung archetypes analysis. \n",
    "<br>\n",
    "Thanks to the editorial team at towardsai.net and Philip Suraman."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f240d264-2021-4547-ad58-ea8e6c003d7f",
   "metadata": {},
   "source": [
    "# <b>Text Mining</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96e793a-3118-46f4-82d0-a5e7ab315135",
   "metadata": {},
   "source": [
    "##### Abstract\n",
    "In this blogpost I will describe and summarizes some basic concepts in text mining. Examples are text preprocessing steps using the NLTK, including tokenization, stemming, lemmatization, POS tagging, named entity recognition, and chunking.\n",
    "<br>\n",
    "\n",
    "The programming environment is: Linux, Python/Jupyter Lab, pandas, numpy and nltk (Natural Language Tool Kit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838bb955-9d09-4fb4-8139-75cbe56eb3ec",
   "metadata": {},
   "source": [
    "### Text Mining\n",
    "Q: What is Text Mining? \n",
    "<br>\n",
    "A: Text Mining is the process of deriving meaningful information from natural language text.\n",
    "\n",
    "Today only 20 percent of the data is being generated in structured format as spoken language.\n",
    "Which means 80% is generated by various means of text, like tweets, WhatsApp, Email, Facebook, Instagram, or any text messages etc. \n",
    "And the majority of this data exists in a textual form often mentioned as \"unstructured format\".\n",
    "\n",
    "\n",
    "Communicating and sharing information, this is where the concepts of language come into the picture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1797ee83-7d1a-4e75-b392-5fb634fddd59",
   "metadata": {},
   "source": [
    "### Natural Language Processing (NLP)\n",
    "Q: What is NLP? \n",
    "<br>\n",
    "A: Natural Language Processing(NLP) is a field in of computer science and artificial intelligence which deals with human language.\n",
    "\n",
    "There are many languages in the world and each language has its own standards and alphabets - each language has a combination of words arranged which results in the formation of a sentence a human kan understand. \n",
    "Each language has its own rules to develop sentences and these sets of rules are also known as grammar.\n",
    "\n",
    "In order to produce meaningful insights from the text data, we need to follow a method called Text Analysis. Natural Language Processing is a component of text mining that performs a special kind of linguistic analysis (Text Analysis) that essentially helps a machine interpret text and create context.\n",
    "\n",
    "It uses different methodologies to decipher language ambiguities in human language and we will go through this processes step-by-step using Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b1a706-9ee7-4fba-8f21-31ef4e18bb02",
   "metadata": {},
   "source": [
    "First, we need to install the NLTK library that is the Natural Language Toolkit for building Python programs to work with human language data, and it also provides a easy to use interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f775e3b-c288-40a6-accf-9a2a234e004d",
   "metadata": {},
   "source": [
    "## Terminologies in NLP\n",
    "Tokenization\n",
    "\n",
    "Tokenization is the first step in NLP. It is the process of breaking strings into tokens, which in turn are small structures or units. Tokenization involves three steps, which are breaking a complex sentence into words, understanding the importance of each word with respect to the sentence, and finally produce a structural description on an input sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e205bef5-5d41-4b56-a522-b0223e08f17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "#import nltk.corpus    # Corpus is an already made text from NLTK, I am not using that - making my own instead.\n",
    "text = 'In Scandinavia they drive on the right-hand side of the road. Scandinavia has a large coastline on both the eastern and western side'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "29b1371e-eacc-4691-8cdf-faca35185558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/pa-on-\n",
      "[nltk_data]     vajert/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LookupError: \n",
    "#**********************************************************************\n",
    "#  Resource punkt not found.\n",
    "# SOLUTION to ERROR:\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fac06894-d7e5-45f5-b85a-7ca6c9733fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'Scandinavia',\n",
       " 'they',\n",
       " 'drive',\n",
       " 'on',\n",
       " 'the',\n",
       " 'right-hand',\n",
       " 'side',\n",
       " 'of',\n",
       " 'the',\n",
       " 'road',\n",
       " '.',\n",
       " 'Scandinavia',\n",
       " 'has',\n",
       " 'a',\n",
       " 'large',\n",
       " 'coastline',\n",
       " 'on',\n",
       " 'both',\n",
       " 'the',\n",
       " 'eastern',\n",
       " 'and',\n",
       " 'western',\n",
       " 'side']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing word_tokenize from nltk\n",
    "from nltk.tokenize import word_tokenize# Passing the string text into word tokenize for breaking the sentences\n",
    "token = word_tokenize(text)\n",
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dcc44d-96b8-4a54-962b-79da30a0f69f",
   "metadata": {},
   "source": [
    "From the above output, we can see the text split into tokens. Words, comma, punctuations are called tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36214c3c-cafe-490b-bbdd-21081fdd60f3",
   "metadata": {},
   "source": [
    "## Finding frequency distinct in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "67d173cd-ba5e-4ba0-8c1b-3738e00dd431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 3, 'Scandinavia': 2, 'on': 2, 'side': 2, 'In': 1, 'they': 1, 'drive': 1, 'right-hand': 1, 'of': 1, 'road': 1, ...})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding the frequency distinct in the tokens\n",
    "# Importing FreqDist library from nltk and passing token into FreqDist\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(token)\n",
    "#fdist\n",
    "fdist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdf9f51-dc6d-44c4-9a93-9dc7ccb6a6cc",
   "metadata": {},
   "source": [
    "‘the’ is found 3 times in the text, 'Scandinavia' is found 2 times in the text, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6e7153e6-178e-4ed7-93bb-1dd702da3578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 3),\n",
       " ('Scandinavia', 2),\n",
       " ('on', 2),\n",
       " ('side', 2),\n",
       " ('In', 1),\n",
       " ('they', 1),\n",
       " ('drive', 1),\n",
       " ('right-hand', 1),\n",
       " ('of', 1),\n",
       " ('road', 1)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To find the frequency of top 10 words\n",
    "fdistmost = fdist.most_common(10)\n",
    "fdistmost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3fc97c-b037-4970-93e7-1c15ddf995f2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbe36f7f-aac2-4aa3-b3c9-157f830fa675",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Stemming\n",
    "Stemming usually refers to normalizing words into its base form or root form.\n",
    "\n",
    "Here, we have words waited, waiting, and waits. Here the root word is ‘wait.’ \n",
    "<br>\n",
    "There are two methods in Stemming: Porter Stemming (removes common morphological and inflectional endings from words) and Lancaster Stemming (a more aggressive stemming algorithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a59db019-e81e-40e6-a154-eb5c8f079d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'erupt'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing Porterstemmer from nltk library\n",
    "# Checking for the word ‘erupting’ \n",
    "from nltk.stem import PorterStemmer\n",
    "pst = PorterStemmer()\n",
    "pst.stem('erupting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "28f3e3c6-002d-4cb3-9d0f-3059cc7418f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erupted:erupt\n",
      "erupting:erupt\n",
      "erupts:erupt\n"
     ]
    }
   ],
   "source": [
    "# Checking for the list of words\n",
    "stm = ['erupted', 'erupting', 'erupts']\n",
    "for word in stm :\n",
    "   print(word+ \":\" +pst.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ddb1f7bb-0730-4471-8dcf-9fddb266b8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "giving:giv\n",
      "given:giv\n",
      "given:giv\n",
      "gave:gav\n"
     ]
    }
   ],
   "source": [
    "# Importing LancasterStemmer from nltk\n",
    "# Using the word ‘giving’ \n",
    "from nltk.stem import LancasterStemmer\n",
    "lst = LancasterStemmer()\n",
    "stm = ['giving', 'given', 'given', 'gave']\n",
    "for word in stm :\n",
    " print(word+ ':' +lst.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4000db-040d-400d-baeb-f3f6b20fce59",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "In simpler terms, it is the process of converting a word to its base form. The difference between stemming and lemmatization is that lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, often leading to incorrect meanings and spelling errors.\n",
    "\n",
    "For example, lemmatization would correctly identify the base form of ‘caring’ to ‘care,’ whereas stemming would cutoff the ‘ing’ part and convert it into a car.\n",
    "\n",
    "Lemmatization can be implemented in python by using Wordnet Lemmatizer, Spacy Lemmatizer, TextBlob, Stanford CoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d56efd58-a389-480a-8170-7e8b3c078bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n"
     ]
    }
   ],
   "source": [
    "# Importing Lemmatizer library from nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Solution given in errorstack.\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "print('rocks :', lemmatizer.lemmatize('rocks')) \n",
    "print('corpora :', lemmatizer.lemmatize('corpora'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4276ad8e-7db3-4e1b-89a5-095cee261048",
   "metadata": {},
   "source": [
    "## Stop Words\n",
    "\n",
    "“Stop words” are the most common words in a language like “the”, “a”, “at”, “for”, “above”, “on”, “is”, “all”. \n",
    "<br>\n",
    "These words do not provide any meaning and are usually removed from texts. We can remove these stop words using nltk library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bcc108bb-5a34-4ebd-bb1e-1662046aa1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/pa-on-\n",
      "[nltk_data]     vajert/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#You can view the list of included stop words in NLTK using the code below:\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "#print(stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b2ef7c-1a7d-461d-9f43-6689530916b7",
   "metadata": {},
   "source": [
    "You can do that for different languages, so you can configure for the language you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2ac71df-e774-4e40-8e04-985536cf9b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('german'))\n",
    "stops = set(stopwords.words('swedish'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b149cd-d02b-4ea4-be0b-e2ba35dcf1d7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72018812-1d30-4fcd-832b-7f8fb9660945",
   "metadata": {},
   "source": [
    "## Filter stop words nltk\n",
    "\n",
    "We will use a string (data) as text. Of course you can also do this with a text file as input. If you want to use a text file instead, you can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "84c8468d-8d4c-4ffc-b624-8d478a8383b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any text here as a txt-file, I have used the lyrics of the song Kashmir by Led Zeppelin.\n",
    "text = open('/home/pa-on-vajert/2022_IBM_Pandas_SCB_TrV_Notebooks/2022_November_Jung_NLP_Textmining_Jung/led_zeppelin_kashmir.txt').read().lower()   # Led Zeppelin Kashmir Lyrics on a txt-file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20710959-bc03-422f-89b3-cbc662362864",
   "metadata": {},
   "source": [
    "### The program below filters stop words from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "141e18fc-d806-4be9-83d7-1e0bc0b364d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'saw', 'best', 'minds', 'generation', 'die', 'quick', 'fix', 'dark', 'alley', 'streets', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "data = \"I saw the best minds in my generation die from a quick fix in  dark alley streets.\"\n",
    "stopWords = set(stopwords.words('english'))\n",
    "words = word_tokenize(data)\n",
    "wordsFiltered = []\n",
    "\n",
    "for w in words:\n",
    "    if w not in stopWords:\n",
    "        wordsFiltered.append(w)\n",
    "\n",
    "print(wordsFiltered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe72ab7-7390-4400-ade4-6af1efe361e1",
   "metadata": {},
   "source": [
    "A module has been imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f3b73a2b-4520-433f-87c7-b7336a73a3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d7c04e-d5a3-42ba-a2c7-39e2a2d78874",
   "metadata": {},
   "source": [
    "We get a set of English stop words using the line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60f37054-a123-4e10-99f0-b4ca9aeecd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "#stopWords = set(stopwords.words('swedish'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afaefd9-6927-48fd-9bae-6f01bfbe9793",
   "metadata": {},
   "source": [
    "The returned list stopWords contains 179 stop words on my computer.\n",
    "You can view the length or contents of this array with the lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e4a4f9fc-81a8-4500-8b1d-85f580c2b554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "{'further', 'my', 'off', 's', 'a', 'myself', 'mustn', 'once', 'why', 'from', 'him', 'very', 'to', \"should've\", 'what', 'shan', 'shouldn', 'couldn', 'and', 'down', 'our', 'll', 'nor', 'doing', 'as', \"you'd\", \"wasn't\", \"mustn't\", 'should', 'few', 'any', 'but', \"you'll\", \"you've\", 'it', 'o', 'itself', 'out', 'ours', 'yourselves', 'its', 'have', 'all', \"didn't\", \"isn't\", 'isn', \"shan't\", 'weren', \"it's\", 'm', 'had', 'having', 'ain', 'so', 't', 'own', 'these', 'haven', 'their', 'more', 'after', 'wouldn', 'is', 'was', \"shouldn't\", 'who', 'her', 'here', 'y', 'only', 'there', \"don't\", 'most', 'yours', 'wasn', 'in', 'above', 'hers', 'against', \"weren't\", 'me', 'some', 'does', 'same', 'can', 'do', 'over', 'or', 'under', 'theirs', 'did', 'you', 'both', 've', 'this', 'i', 'that', 'until', 'on', 'we', \"needn't\", \"wouldn't\", 'at', 'where', 'just', 'then', 'has', 'she', 'been', 'won', 'needn', 'd', \"you're\", 'them', 'if', 'about', 'how', 'before', \"couldn't\", 'mightn', 'doesn', 're', 'no', 'yourself', 'an', 'with', \"doesn't\", 'when', \"that'll\", 'other', 'such', 'into', 'between', 'ourselves', \"hadn't\", \"hasn't\", \"haven't\", \"won't\", 'now', 'being', 'not', 'which', 'hasn', 'don', 'by', 'through', 'are', 'below', 'during', 'because', 'he', 'for', 'up', 'hadn', 'too', \"she's\", 'again', 'of', 'himself', \"aren't\", 'were', 'the', 'his', 'while', 'am', 'your', 'be', 'themselves', 'they', 'each', 'will', 'aren', 'didn', 'those', 'ma', 'whom', \"mightn't\", 'herself', 'than'}\n"
     ]
    }
   ],
   "source": [
    "print(len(stopWords))\n",
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb124e0-1894-4616-9735-b60d2eaf3015",
   "metadata": {},
   "source": [
    "We create a new list called wordsFiltered which contains all words which are not stop words.\n",
    "To create it we iterate over the list of words and only add it if its not in the stopWords list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "32bce65e-f873-4177-8d78-cbdf090e7aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "    if w not in stopWords:\n",
    "        wordsFiltered.append(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7641a628-2e20-49fe-83e8-5fbda335f401",
   "metadata": {},
   "source": [
    "## Part of speech tagging (POS)\n",
    "\n",
    "Part-of-speech tagging is used to assign parts of speech to each word of a given text (such as nouns, verbs, pronouns, adverbs, conjunction, adjectives, interjection) based on its definition and its context. There are many tools available for POS taggers, and some of the widely used taggers are NLTK, Spacy, TextBlob, Standford CoreNLP, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9f6cc02e-09a5-481b-9c04-511f3ae0fc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/pa-on-vajert/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LookupError: \n",
    "#  Resource averaged_perceptron_tagger not found.\n",
    "#  Please use the NLTK Downloader to obtain the resource:\n",
    "#import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c9cd11f1-1fbf-4d64-8ef3-e8a46b0e7276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Elvis', 'NN')]\n",
      "[('Aaron', 'NNP')]\n",
      "[('Presley', 'NN')]\n",
      "[('was', 'VBD')]\n",
      "[('born', 'NN')]\n",
      "[('on', 'IN')]\n",
      "[('January', 'NNP')]\n",
      "[('8', 'CD')]\n",
      "[(',', ',')]\n",
      "[('1935', 'CD')]\n",
      "[(',', ',')]\n",
      "[('in', 'IN')]\n",
      "[('Tupelo', 'NN')]\n",
      "[(',', ',')]\n",
      "[('Mississippi', 'NNP')]\n",
      "[(',', ',')]\n",
      "[('USA', 'NNP')]\n",
      "[(',', ',')]\n",
      "[('and', 'CC')]\n",
      "[('died', 'VBD')]\n",
      "[('August', 'NNP')]\n",
      "[('16', 'CD')]\n",
      "[(',', ',')]\n",
      "[('1977', 'CD')]\n",
      "[('(', '(')]\n",
      "[('aged', 'VBN')]\n",
      "[('42', 'CD')]\n",
      "[(')', ')')]\n",
      "[('in', 'IN')]\n",
      "[('Memphis', 'NN')]\n",
      "[(',', ',')]\n",
      "[('Tennessee', 'NN')]\n",
      "[(',', ',')]\n",
      "[('The', 'DT')]\n",
      "[('cause', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('death', 'NN')]\n",
      "[('was', 'VBD')]\n",
      "[('Heart', 'NN')]\n",
      "[('disease', 'NN')]\n",
      "[('due', 'JJ')]\n",
      "[('to', 'TO')]\n",
      "[('a', 'DT')]\n",
      "[('kingly', 'RB')]\n",
      "[('Rock-and-roll', 'NN')]\n",
      "[('living', 'NN')]\n",
      "[(',', ',')]\n",
      "[('including', 'VBG')]\n",
      "[('banana', 'NN')]\n",
      "[('peanut', 'NN')]\n",
      "[('butter', 'NN')]\n",
      "[('sandwich', 'NN')]\n",
      "[('sandwich', 'NN')]\n",
      "[(',', ',')]\n",
      "[('fried', 'VBN')]\n",
      "[('ducklings', 'NNS')]\n",
      "[('with', 'IN')]\n",
      "[('ice', 'NN')]\n",
      "[('cream', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('lot', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('Plum', 'NN')]\n",
      "[('in', 'IN')]\n",
      "[('Madeira', 'NN')]\n",
      "[('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "text = 'Elvis Aaron Presley was born on January 8, 1935, in Tupelo, Mississippi, USA, and died August 16, 1977 (aged 42) in Memphis, Tennessee, The cause of death was Heart disease due to a kingly Rock-and-roll living, including banana peanut butter sandwich sandwich, fried ducklings with ice cream a lot of Plum in Madeira.'\n",
    "#Tokenize the text\n",
    "tex = word_tokenize(text)\n",
    "for token in tex:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e717cd-638e-4925-a9c9-e0b6a8703afa",
   "metadata": {},
   "source": [
    "## Named entity recognition\n",
    "\n",
    "It is the process of detecting the named entities such as the person name, the location name, the company name, the quantities, and the monetary value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1658626a-e939-4663-8a2b-fe49b1424235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to /home/pa-on-\n",
      "[nltk_data]     vajert/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/pa-on-\n",
      "[nltk_data]     vajert/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LookupError: \n",
    "#  Resource maxent_ne_chunker not found.\n",
    "#  Please use the NLTK Downloader to obtain the resource:\n",
    "#import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c682cbc6-6856-4aa5-806b-1602a48e911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import svgling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "88763472-f90b-43ec-b3df-528364e801a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"120px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,1224.0,120.0\" width=\"1224px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"2.61438%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">At</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"1.30719%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.92157%\" x=\"2.61438%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">its</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">PRP$</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"4.57516%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.92157%\" x=\"6.53595%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">most</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"8.49673%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"2.61438%\" x=\"10.4575%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">i</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"11.7647%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.26797%\" x=\"13.0719%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">had</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"14.7059%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"4.57516%\" x=\"16.3399%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">three</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"18.6275%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.92157%\" x=\"20.915%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">cute</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"22.8758%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"5.88235%\" x=\"24.8366%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">kittens</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBZ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"27.7778%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"5.22876%\" x=\"30.719%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">living</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBG</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"33.3333%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"5.88235%\" x=\"35.9477%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">indoors</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"38.8889%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"1.96078%\" x=\"41.8301%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">-</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">:</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"42.8105%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"2.61438%\" x=\"43.7908%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">so</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">RB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"45.098%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"2.61438%\" x=\"46.4052%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">no</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"47.7124%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.92157%\" x=\"49.0196%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">rats</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50.9804%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"4.57516%\" x=\"52.9412%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">dared</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"55.2288%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"2.61438%\" x=\"57.5163%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">to</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">TO</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"58.8235%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"4.57516%\" x=\"60.1307%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">dande</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"62.4183%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"2.61438%\" x=\"64.7059%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">on</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"66.0131%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.26797%\" x=\"67.3203%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"68.9542%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"5.88235%\" x=\"70.5882%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">kitchen</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"73.5294%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"4.57516%\" x=\"76.4706%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">table</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"78.7582%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"5.22876%\" x=\"81.0458%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">during</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"83.6601%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.92157%\" x=\"86.2745%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">that</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"88.2353%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"6.53595%\" x=\"90.1961%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">glorious</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"93.4641%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.26797%\" x=\"96.732%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">era</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"98.366%\" y1=\"1.2em\" y2=\"3em\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [('At', 'IN'), ('its', 'PRP$'), ('most', 'JJS'), ('i', 'NN'), ('had', 'VBD'), ('three', 'CD'), ('cute', 'NN'), ('kittens', 'VBZ'), ('living', 'VBG'), ('indoors', 'NNS'), ('-', ':'), ('so', 'RB'), ('no', 'DT'), ('rats', 'NNS'), ('dared', 'VBD'), ('to', 'TO'), ('dande', 'VB'), ('on', 'IN'), ('the', 'DT'), ('kitchen', 'NN'), ('table', 'NN'), ('during', 'IN'), ('that', 'DT'), ('glorious', 'JJ'), ('era', 'NN')])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text = 'Som mest hade jag inneboende tre tre söta missar - så ingen råtta vågade dansa på bordet under den tiden'\n",
    "text = 'At its most i had three cute kittens living indoors - so no rats dared to dande on the kitchen table during that glorious era '\n",
    "from nltk import ne_chunk# tokenize and POS Tagging before doing chunk\n",
    "token = word_tokenize(text)\n",
    "tags = nltk.pos_tag(token)\n",
    "chunk = ne_chunk(tags)\n",
    "chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfec72e-9a36-4715-b9fb-33d860e64a72",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "Chunking means picking up individual pieces of information and grouping them into bigger pieces. In the context of NLP and text mining, chunking means a grouping of words or tokens into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2960523a-d5d6-4754-b4d7-011f65852540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  At/IN\n",
      "  its/PRP$\n",
      "  most/JJS\n",
      "  (NP i/NN)\n",
      "  had/VBD\n",
      "  three/CD\n",
      "  (NP cute/NN)\n",
      "  kittens/VBZ\n",
      "  living/VBG\n",
      "  indoors/NNS\n",
      "  -/:\n",
      "  so/RB\n",
      "  no/DT\n",
      "  rats/NNS\n",
      "  dared/VBD\n",
      "  to/TO\n",
      "  dande/VB\n",
      "  on/IN\n",
      "  (NP the/DT kitchen/NN)\n",
      "  (NP table/NN)\n",
      "  during/IN\n",
      "  (NP that/DT glorious/JJ era/NN))\n"
     ]
    }
   ],
   "source": [
    "text = 'At its most i had three cute kittens living indoors - so no rats dared to dande on the kitchen table during that glorious era '\n",
    "token = word_tokenize(text)\n",
    "tags = nltk.pos_tag(token)\n",
    "reg = 'NP: {<DT>?<JJ>*<NN>}' \n",
    "a = nltk.RegexpParser(reg)\n",
    "result = a.parse(tags)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312227db-0563-4540-9272-0503737b7371",
   "metadata": {},
   "source": [
    "<br>\n",
    "This blog summarizes text preprocessing and covers the NLTK steps, including Tokenization, Stemming, Lemmatization, POS tagging, Named entity recognition, and Chunking.\n",
    "<br>\n",
    "<br>\n",
    "Keep on coding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcf9988-057f-4d30-9aeb-7aa9f0ada753",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
