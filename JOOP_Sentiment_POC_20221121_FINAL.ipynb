{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bdfe411-e34d-4a07-ad96-1de5cfcbff7f",
   "metadata": {},
   "source": [
    "\n",
    "JAAP (Jung Archetypes Analytical Project Text-Mining-II-II)\n",
    "\n",
    "\n",
    "# Jung Archetypes Analytical Project Text Mining II-II, a continuation of last weeks project, building contextâ€¦\n",
    "\n",
    "\n",
    "Text Classification with Python and Scikit-Learn\n",
    "\n",
    "Text classification is one of the most commonly used NLP tasks. This is an example of sentimental analysis of movie reviews, a common text classification task being performed using Python, SciKit etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f0a147-f8ac-4eeb-90ec-d390064b0107",
   "metadata": {},
   "source": [
    "## Text Classification with Python and Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454ca92b-0e1b-4e00-84e4-8fe436a006f6",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Text classification is one of the most commonly used NLP tasks. \n",
    "\n",
    "It is the process of classifying text strings or documents into different categories, depending upon the contents of the strings. Text classification has a variety of applications, such as detecting user sentiment from a tweet, classifying an email as spam, or classifying blog posts into different categories, automatic tagging of customer queries, etc.\n",
    "\n",
    "This is a example of text classification where I will train a machine learning model capable of predicting whether a given movie review is positive or negative.\n",
    "\n",
    "An example of sentimental analysis where viewers sentiments towards a particular entity are classified into different categories, a common text classification task being performed using Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "405c7518-aa2c-4bb7-8820-8527e6bc2841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/pa-on-\n",
      "[nltk_data]     vajert/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.datasets import load_files\n",
    "nltk.download('stopwords')\n",
    "import pickle\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f91e120-f1bf-4c0e-b2c4-68ddd0aadf12",
   "metadata": {},
   "source": [
    "### The dataset\n",
    "The dataset being used can be downloaded from the Cornell Natural Language Processing Group. The dataset consists of a total of 2000 documents. Half of the documents contain positive reviews regarding a movie while the remaining half contains negative reviews.\n",
    "\n",
    "Further details regarding the dataset can be found at this link."
   ]
  },
  {
   "cell_type": "raw",
   "id": "2a985dab-b319-40c8-9f4e-4de3fa499fe8",
   "metadata": {},
   "source": [
    "# link to saved data on your HD.\n",
    "movie_data = load_files(r\"qwerty - qwerty - qwerty - /txt_sentoken/\")\n",
    "\n",
    "X, y = movie_data.data, movie_data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b106ffe9-e22d-4e45-8b94-97478ddfef35",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "\n",
    "Once the dataset has been imported, the next step is to preprocess the text. \n",
    "\n",
    "Text may contain numbers, special characters, and unwanted spaces. Depending upon the problem we face, we may or may not need to remove these special characters and numbers from text. \n",
    "\n",
    "However, for the sake of explanation, we will remove all the special characters, numbers, and unwanted spaces from our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af179ff7-e0b4-4a68-a195-acd1d3ccaf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbb102b-7213-4a00-969d-8c33ad5c283e",
   "metadata": {},
   "source": [
    "### Regex Expressions\n",
    "In the script above we use Regex Expressions from Python re library to perform different preprocessing tasks. We start by removing all non-word characters such as special characters, numbers, etc.\n",
    "\n",
    "We remove all the single characters.\n",
    "\n",
    "Next, we use the \\^[a-zA-Z]\\s+ regular expression to replace a single character from the beginning of the document, with a single space. Replacing single characters with a single space may result in multiple spaces, which is not ideal.\n",
    "\n",
    "We again use the regular expression \\s+ to replace one or more spaces with a single space.\n",
    "\n",
    "When you have a dataset in bytes format, the alphabet letter \"b\" is appended before every string. The regex ^b\\s+ removes \"b\" from the start of a string. The next step is to convert the data to lower case so that the words that are actually the same but have different cases can be treated equally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebbc818-14d6-4c78-9b17-79fb54ae5002",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "The final preprocessing step is the lemmatization. In lemmatization, we reduce the word into dictionary root form. For instance \"cats\" is converted into \"cat\". Lemmatization is done in order to avoid creating features that are semantically similar but syntactically different. For instance, we don't want two different features named \"cats\" and \"cat\", which are semantically similar, therefore we perform lemmatization.\n",
    "Converting Text to Numbers\n",
    "\n",
    "Machines, unlike humans, cannot understand the raw text. Machines can only see numbers. Particularly, statistical techniques such as machine learning can only deal with numbers. Therefore, we need to convert our text into numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5594660b-6753-4d79-b2c6-3f5a728f31f1",
   "metadata": {},
   "source": [
    "### Bag of Words Model\n",
    "Different approaches exist to convert text into the corresponding numerical form. The Bag of Words Model and the Word Embedding Model are two of the most commonly used approaches. In this article, we will use the bag of words model to convert our text to numbers.\n",
    "Bag of Words\n",
    "\n",
    "The following script uses the bag of words model to convert text documents into corresponding numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47f060f0-5184-4ed6-bf3a-1faf580cca97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also directly convert text documents into TFIDF feature values \n",
    "#(without first converting documents to bag of words features) using the following script:\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(documents).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c17db7-9dc2-4397-832f-c840aad2bfda",
   "metadata": {},
   "source": [
    "### Bag of Words Model II-II\n",
    "The script above uses CountVectorizer class from the sklearn.feature_extraction.text library. There are some important parameters that are required to be passed to the constructor of the class. The first parameter is the max_features parameter, which is set to 1500. This is because when you convert words to numbers using the bag of words approach, all the unique words in all the documents are converted into features. All the documents can contain tens of thousands of unique words. But the words that have a very low frequency of occurrence are unusually not a good parameter for classifying documents. Therefore we set the max_features parameter to 1500, which means that we want to use 1500 most occurring words as features for training our classifier.\n",
    "\n",
    "The next parameter is min_df and it has been set to 5. This corresponds to the minimum number of documents that should contain this feature. So we only include those words that occur in at least 5 documents. Similarly, for the max_df, feature the value is set to 0.7; in which the fraction corresponds to a percentage. Here 0.7 means that we should include only those words that occur in a maximum of 70% of all the documents. Words that occur in almost every document are usually not suitable for classification because they do not provide any unique information about the document.\n",
    "\n",
    "Finally, we remove the stop words from our text since, in the case of sentiment analysis, stop words may not contain any useful information. To remove the stop words we pass the stopwords object from the nltk.corpus library to the stop_wordsparameter.\n",
    "\n",
    "The fit_transform function of the CountVectorizer class converts text documents into corresponding numeric features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07388a48-48c9-4662-a4f9-59b50c1e234c",
   "metadata": {},
   "source": [
    "### Finding TFIDF\n",
    "\n",
    "The bag of words approach works fine for converting text to numbers. However, it has one drawback. It assigns a score to a word based on its occurrence in a particular document. It doesn't take into account the fact that the word might also be having a high frequency of occurrence in other documents as well. TFIDF resolves this issue by multiplying the term frequency of a word by the inverse document frequency. The TF stands for \"Term Frequency\" while IDF stands for \"Inverse Document Frequency\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c62afd-6507-4940-869d-b7ba6f2e5d5c",
   "metadata": {},
   "source": [
    "The TFIDF value for a word in a particular document is higher if the frequency of occurrence of that word is higher in that specific document but lower in all the other documents.\n",
    "\n",
    "To convert values obtained using the bag of words model into TFIDF values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b033877d-3be4-4301-8c40-de9e84a9d58e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.feature_extraction.text import TfidfTransformer\\ntfidfconverter = TfidfTransformer()\\nX = tfidfconverter.fit_transform(X).toarray()\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Term frequency = (Number of Occurrences of a word)/(Total words in the document)\n",
    "#IDF(word) = Log((Total number of documents)/(Number of documents containing the word))\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidfconverter = TfidfTransformer()\n",
    "X = tfidfconverter.fit_transform(X).toarray()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc701e5-b42f-4303-8f33-1c49ba51f98a",
   "metadata": {},
   "source": [
    "### Training and Testing Sets\n",
    "\n",
    "Like any other supervised machine learning problem, we need to divide our data into training and testing sets. To do so, we will use the train_test_split utility from the sklearn.model_selection library. Execute the following script:\n",
    "\n",
    "The above script divides data into 20% test set and 80% training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c53f2c8f-ea44-4a8d-8a68-04bcd8a3b915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d2f477-7150-4c21-b94d-4a4d8ecb431a",
   "metadata": {},
   "source": [
    "### Training Text Classification Model and Predicting Sentiment\n",
    "\n",
    "We have divided our data into training and testing set. Now is the time to see the real action. We will use the Random Forest Algorithm to train our model. You can you use any other model of your choice.\n",
    "\n",
    "To train our machine learning model using the random forest algorithm we will use RandomForestClassifier class from the sklearn.ensemble library. The fit method of this class is used to train the algorithm. We need to pass the training data and training target sets to this method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db0551ae-88e4-422e-9939-7f2717acb04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "221cfd3e-3450-40fe-9ede-1dd83be1fb55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=1000, random_state=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be5ccb7f-fef2-47a1-9567-b5e8d7bdd8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f409fa10-a44c-4b8c-85f3-98154cf554d0",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6806c3f9-d25d-47b6-bfda-1c1521c70d15",
   "metadata": {},
   "source": [
    "To evaluate the performance of a classification model such as the one that we just trained, we can use metrics such as the confusion matrix, F1 measure, and the accuracy.\n",
    "\n",
    "To find these values, we can use classification_report, confusion_matrix, and accuracy_score utilities from the sklearn.metrics library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "beb593be-ae6c-42a8-b330-ef5149cd0445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[167  41]\n",
      " [ 19 173]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.80      0.85       208\n",
      "           1       0.81      0.90      0.85       192\n",
      "\n",
      "    accuracy                           0.85       400\n",
      "   macro avg       0.85      0.85      0.85       400\n",
      "weighted avg       0.85      0.85      0.85       400\n",
      "\n",
      "0.85\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403261cb-5dd1-492d-a6af-d319a6413d65",
   "metadata": {},
   "source": [
    "From the output, it can be seen that our model achieved an accuracy of 85.5%, which is very good given the fact that we randomly chose all the parameters for CountVectorizer as well as for our random forest algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e5af32-9f1f-409e-acb4-a81acba4ca21",
   "metadata": {},
   "source": [
    "## Saving and Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "069565f1-66fd-40f1-b3d3-69836be859d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text_classifier', 'wb') as picklefile:\n",
    "    pickle.dump(classifier,picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f42cba2-5b84-4505-86f6-4434dda2f6d3",
   "metadata": {},
   "source": [
    "Once you execute the above script, you can see the text_classifier file in your working directory. We have saved our trained model and we can use it later for directly making predictions, without training.\n",
    "\n",
    "To load the model, we can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76c6c4eb-3481-4694-a54d-c2c5be260a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text_classifier', 'rb') as training_model:\n",
    "    model = pickle.load(training_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b9082c-4451-4a40-be84-47744abe9cf9",
   "metadata": {},
   "source": [
    "We loaded our trained model and stored it in the model variable. Let's predict the sentiment for the test set using our loaded model and see if we can get the same results. Execute the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74fa2980-f1e0-4e2d-8745-b0ce966bf1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[158  50]\n",
      " [ 46 146]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.76      0.77       208\n",
      "           1       0.74      0.76      0.75       192\n",
      "\n",
      "    accuracy                           0.76       400\n",
      "   macro avg       0.76      0.76      0.76       400\n",
      "weighted avg       0.76      0.76      0.76       400\n",
      "\n",
      "0.76\n"
     ]
    }
   ],
   "source": [
    "y_pred2 = model.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred2))\n",
    "print(classification_report(y_test, y_pred2))\n",
    "print(accuracy_score(y_test, y_pred2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72339fcd-b8f5-47ed-86f1-72385104e644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01681aec-4775-4f5c-b15b-154dcc51dd34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9732818b-1394-4353-9241-e1f6326c327d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
